
目标: 北京链家所有二手房数据

实现的功能: 随机代理, 使用bloomfilter去重, scrapyd远程监测控制, docker批量部署


思路:
linkspider
请求所有大区域, 获得所有小区域链接
请求所有小区域, 获得该小区域总列表页数
构造链接并存入redis_key "bj_ershouf:start_urls"中

ershoufspider
请求构造好的每个列表页
rules规则抓取列表页的详情页
解析数据并存入mongoDB中


单机运行
启动链接爬虫
scrapy crawl bj_link
给链接发布任务
lpush bj_ershouf_region:start_urls https://bj.lianjia.com/ershoufang/
启动二手房信息爬虫
scrapy crawl bj_ershouf



分布式

scrapyd构建部分

如果要对接docker, 先修改settings.py中的
MONGO_URI = '172.17.0.1'
REDIS_HOST = '172.17.0.1'


1. pip install scrapyd && pip install scrapyd-client
2. 配置Lib\site-packages\scrapyd\default_scrapyd.conf(如下docker部分的新建scrapyd.conf)
3. 将项目打包成egg文件  scrapyd-deploy --version [id]
4. 将egg文件通过addversion.json部署到运行着scrapyd主机上(本地不用)
curl http://localhost:6800/addversion.json -F project=s_redis -F version= [id]

遇到的问题在win虚拟环境内没有生成eggs文件, 请在当前工作路径运行scrapyd


开始指定爬虫项目
curl http://localhost:6800/schedule.json -d project=lianjia_ershouf_web -d spider=bj_link
curl http://localhost:6800/schedule.json -d project=lianjia_ershouf_web -d spider=bj_ershouf
结束指定爬虫任务
curl http://localhost:6800/cancel.json -d project=lianjia_ershouf_web -d job=d2203124d6d411e8a619089e018cd9b7




docker构建部分

首先安装docker, 安装docker教程和方法很多这里就不说了

添加scrapyd.conf文件
添加requirement.txt
添加Dockerfile文件
docker部分


新建scrapyd.conf

[scrapyd]
eggs_dir = eggs
logs_dir = logs
items_dir = 
jobs_to_keep =5
dbs_dir = dbs
max_proc = 0
max_proc_per_cpu = 10
finished_to_keep = 100
poll_interval = 5.0
bind_address = 0.0.0.0
http_port = 6800
debug = off
runner = scrapyd.runner
application = scrapyd.app.application
launcher = scrapyd.launcher.Launcher
webroot = scrapyd.website.Root


[services]
schedule.json = scrapyd.webservice.Schedule
cancel.json = scrapyd.webservice.Cancel
addversion.json = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json = scrapyd.webservice. ListSpiders
delproject.json = scrapyd.webservice.DeleteProject
delversion.json = scrapyd.webservice.DeleteVersion
listjobs.json = scrapyd.webservice.ListJobs
daemonstatus.json = scrapyd.webservice.DaemonStatus


生成requirements
pip freeze > requirements.txt



新建Dockerfile

FROM python:3.6
ADD . /code
WORKDIR /code
COPY ./scrapyd.conf /etc/scrapyd/
EXPOSE 6800
RUN pip3 install -i https://pypi.douban.com/simple -r requirements.txt
CMD scrapyd



docker使用部分

为了让docker容器能访问宿主机中的redis和mongoDB
修改主机上的iptables规则以允许来自Docker容器的连接
iptables -A INPUT -i docker0 -j ACCEPT


构建image(注意最后有个小点)
docker build -t bridi/scrapyd:latest .
创建运行容器
docker run -d -p 6800:6800 bridi/scrapyd


开始指定爬虫项目
curl http://47.##.##.##:6800/schedule.json -d project=lianjia_ershouf_web -d spider=bj_link
curl http://47.##.##.##:6800/schedule.json -d project=lianjia_ershouf_web -d spider=bj_ershouf
结束指定爬虫任务
curl http://47.##.##.##:6800/cancel.json -d project=lianjia_ershouf_web -d job=fdb7d5ccd91111e8be820242ac110002


连接云上的redis发布任务
lpush bj_ershouf_region:start_urls https://bj.lianjia.com/ershoufang/


关闭容器
docker stop [containerID] 
再次运行
docker start [containerID] 



如果之前没修改docker中文件, 请在宿主机修改好的settings文件
MONGO_URI = '172.17.0.1'
REDIS_HOST = '172.17.0.1'
复制宿主机修改好的settings文件到docker容器中  容器ID:绝对路径
docker cp settings.py edac42871a9c:/lianjia/lianjia_ershouf_web/settings.py




docker批量部署
打包
docker tag scrapyd:latest bridi/scrapyd:latest
上传到docker hub
docker push bridi/scrapyd:latest
其他主机上运行
docker run -d -p 6800:6800 bridi/scrapyd







构建
https://github.com/stream-wei/Lianjia/blob/master/Lianjia/spiders/lianjia.py
https://github.com/feidaoGavin/diandianzu/blob/master/diandianzu/spiders/lianjia.py
https://github.com/nladuo/lianjia_crawler/tree/master/lianjia_crawler/spiders

反爬
https://www.jianshu.com/p/aeaa8e0f8f60
https://cdn2.jianshu.io/p/396776e26f89
流量异常